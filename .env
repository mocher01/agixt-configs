# =============================================================================
# AGiXT Server Configuration - v1.2-ezlocolai-light
# =============================================================================
# Generated: 2025-06-01T20:00:00
# Features: Nginx Proxy + EzLocalAI + Qwen2.5-1.5B Model + GraphQL
# Domains: https://agixt.locod-ai.com + https://agixtui.locod-ai.com
# Model: Lightweight Qwen2.5-Coder-1.5B (~1GB RAM)
# Optimization: 16GB RAM servers, code generation, workflows
# =============================================================================

# VERSION & BASIC
AGIXT_VERSION=v1.2-ezlocolai-light
INSTALL_DATE=2025-06-01T20:00:00
AGIXT_AUTO_UPDATE=true
AGIXT_API_KEY=YOUR_GENERATED_API_KEY_HERE
UVICORN_WORKERS=6
WORKING_DIRECTORY=./WORKSPACE
TZ=Europe/Paris

# PROXY URLS
AGIXT_SERVER=https://agixt.locod-ai.com
AGIXT_URI=http://agixt:7437
APP_URI=https://agixtui.locod-ai.com
AUTH_WEB=https://agixtui.locod-ai.com/user

# INTERFACE
APP_NAME=AGiXT Production Server v1.2-ezlocolai-light
APP_DESCRIPTION=AGiXT Production Server with EzLocalAI & Qwen2.5-1.5B Model Integration
AGIXT_AGENT=CodeAssistant
AGIXT_SHOW_SELECTION=agent,conversation
AGIXT_SHOW_AGENT_BAR=true
AGIXT_SHOW_APP_BAR=true
AGIXT_CONVERSATION_MODE=select
INTERACTIVE_MODE=chat
THEME_NAME=doom
AGIXT_FOOTER_MESSAGE=AGiXT v1.2-ezlocolai-light - Qwen2.5-1.5B Model Integration

# AUTHENTICATION
AUTH_PROVIDER=magicalauth
CREATE_AGENT_ON_REGISTER=true
CREATE_AGIXT_AGENT=true
ALLOW_EMAIL_SIGN_IN=true

# FEATURES
AGIXT_FILE_UPLOAD_ENABLED=true
AGIXT_VOICE_INPUT_ENABLED=true
AGIXT_RLHF=true
AGIXT_ALLOW_MESSAGE_EDITING=true
AGIXT_ALLOW_MESSAGE_DELETION=true
AGIXT_SHOW_OVERRIDE_SWITCHES=tts,websearch,analyze-user-input

# SYSTEM
DATABASE_TYPE=sqlite
DATABASE_NAME=models/agixt
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s | %(levelname)s | %(message)s
ALLOWED_DOMAINS=*
AGIXT_BRANCH=stable
AGIXT_REQUIRE_API_KEY=false

# GRAPHQL
GRAPHIQL=true
ENABLE_GRAPHQL=true

# EZLOCALAI INTEGRATION
EZLOCALAI_API_URL=http://ezlocalai:8091
EZLOCALAI_API_KEY=agixt-automation-key
EZLOCALAI_MODEL=Qwen2.5-Coder-1.5B-Instruct
EZLOCALAI_MAX_TOKENS=16384
EZLOCALAI_TEMPERATURE=0.3
EZLOCALAI_TOP_P=0.9
EZLOCALAI_VOICE=DukeNukem

# EZLOCALAI SERVER
DEFAULT_MODEL=Qwen2.5-Coder-1.5B-Instruct
LLM_MAX_TOKENS=16384
THREADS=4
GPU_LAYERS=0
WHISPER_MODEL=base.en
IMG_ENABLED=false
AUTO_UPDATE=true

# EXTERNAL SERVICES
TEXTGEN_URI=http://text-generation-webui:5000
N8N_URI=http://n8n-prod:5678

# =============================================================================
# CONFIGURATION NOTES v1.2-ezlocolai-light
# =============================================================================
# üîë SECURITY:
#    - Auto-generated secure API key for JWT authentication
#    - API key requirement disabled for easier setup
#
# üåê PROXY SETUP:
#    - Frontend: https://agixtui.locod-ai.com ‚Üí http://agixtinteractive:3437
#    - Backend: https://agixt.locod-ai.com ‚Üí http://agixt:7437
#    - EzLocalAI API: http://162.55.213.90:8091
#    - EzLocalAI UI: http://162.55.213.90:8502
#
# ü§ñ EZLOCALAI - LIGHTWEIGHT QWEN2.5 MODEL:
#    - Model: Qwen2.5-Coder-1.5B-Instruct (~1GB)
#    - Purpose: Python scripts, bash automation, n8n workflows
#    - Temperature: 0.3 (precise code generation)
#    - Max Tokens: 16384 (long code blocks)
#    - CPU Only: 4 threads (optimized for 16GB RAM)
#    - Memory Usage: ~1.5GB (vs 8GB for 7B model)
#
# üîó INTEGRATIONS:
#    - n8n: Pre-configured for workflow automation
#    - GraphQL: Full management interface
#    - Docker Network: agixt-network for internal communication
#    - EzLocalAI UI: Model management and testing
#
# üéØ MODEL INTEGRATION:
#    - Auto-download: Downloads from HuggingFace if not in backup
#    - HuggingFace-style directory structure created
#    - Config files generated for compatibility
#    - Ready for immediate use
#    - Lightweight for 16GB RAM servers
# =============================================================================
