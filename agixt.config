# =============================================================================
# AGiXT Server Configuration - v1.7-optimized-universal
# =============================================================================
# Generated: 2025-06-03T18:00:00
# Features: Nginx Proxy + EzLocalAI + TinyLlama (Ultra-Light) + GraphQL
# Domains: https://agixt.locod-ai.com + https://agixtui.locod-ai.com
# Model: TinyLlama-1.1B-Chat (Optimized for automation, N8N, workflows)
# Optimization: 16GB RAM servers, ultra-light model support
# Authentication: HuggingFace token for model downloads
# =============================================================================

# =============================================================================
# INSTALLATION PARAMETERS
# =============================================================================
AGIXT_VERSION=v1.7-optimized-universal
INSTALL_FOLDER_PREFIX=agixt
INSTALL_BASE_PATH=/var/apps
AGIXT_AUTO_UPDATE=true
UVICORN_WORKERS=3
WORKING_DIRECTORY=./WORKSPACE
TZ=Europe/Paris

# =============================================================================
# MODEL CONFIGURATION (Ultra-Light - Everything Auto-Detected)
# =============================================================================
MODEL_NAME=tinyllama-1.1b-chat-v1.0
MODEL_BACKUP_PATH=/var/backups/ezlocalai-models-20250603/tinyllama-1.1b-chat-v1.0/model.gguf

# =============================================================================
# AUTHENTICATION
# =============================================================================
HUGGINGFACE_TOKEN=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

# =============================================================================
# PROXY URLS
# =============================================================================
AGIXT_SERVER=https://agixt.locod-ai.com
AGIXT_URI=http://agixt:7437
APP_URI=https://agixtui.locod-ai.com
AUTH_WEB=https://agixtui.locod-ai.com/user

# =============================================================================
# INTERFACE
# =============================================================================
APP_NAME=AGiXT Automation Server v1.7-optimized-universal
APP_DESCRIPTION=AGiXT Server with EzLocalAI - Optimized for N8N, Camunda & Automation
AGIXT_AGENT=AutomationAssistant
AGIXT_SHOW_SELECTION=agent,conversation
AGIXT_SHOW_AGENT_BAR=true
AGIXT_SHOW_APP_BAR=true
AGIXT_CONVERSATION_MODE=select
INTERACTIVE_MODE=chat
THEME_NAME=doom
AGIXT_FOOTER_MESSAGE=AGiXT v1.7 - Automation & Deployment Assistant

# =============================================================================
# AUTHENTICATION
# =============================================================================
AUTH_PROVIDER=magicalauth
CREATE_AGENT_ON_REGISTER=true
CREATE_AGIXT_AGENT=true
ALLOW_EMAIL_SIGN_IN=true

# =============================================================================
# FEATURES
# =============================================================================
AGIXT_FILE_UPLOAD_ENABLED=true
AGIXT_VOICE_INPUT_ENABLED=true
AGIXT_RLHF=true
AGIXT_ALLOW_MESSAGE_EDITING=true
AGIXT_ALLOW_MESSAGE_DELETION=true
AGIXT_SHOW_OVERRIDE_SWITCHES=tts,websearch,analyze-user-input

# =============================================================================
# SYSTEM
# =============================================================================
DATABASE_TYPE=sqlite
DATABASE_NAME=models/agixt
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s | %(levelname)s | %(message)s
ALLOWED_DOMAINS=*
AGIXT_BRANCH=stable
AGIXT_REQUIRE_API_KEY=false

# =============================================================================
# GRAPHQL
# =============================================================================
GRAPHIQL=true
ENABLE_GRAPHQL=true

# =============================================================================
# EZLOCALAI INTEGRATION (User Preferences Only)
# =============================================================================
EZLOCALAI_API_URL=http://ezlocalai:8091
EZLOCALAI_TEMPERATURE=0.3
EZLOCALAI_TOP_P=0.9
EZLOCALAI_VOICE=DukeNukem

# =============================================================================
# EZLOCALAI SERVER (Hardware & User Preferences)
# =============================================================================
THREADS=4
GPU_LAYERS=0
WHISPER_MODEL=base.en
IMG_ENABLED=false
AUTO_UPDATE=true

# =============================================================================
# EXTERNAL SERVICES
# =============================================================================
TEXTGEN_URI=http://text-generation-webui:5000
N8N_URI=http://n8n-prod:5678

# =============================================================================
# CONFIGURATION NOTES v1.7-optimized-universal
# =============================================================================
# üîë MODEL CONFIGURATION:
#    - MODEL_NAME: Simple name - script auto-detects everything else
#    - TinyLlama: ~800MB RAM, perfect for automation tasks
#    - Supported: tinyllama-1.1b-chat-v1.0, phi-2, deepseek-coder-1.3b-instruct,
#      llama-2-7b-chat, mistral-7b-instruct-v0.1, codellama-7b-instruct, etc.
#    - Script automatically:
#      * Finds GGUF versions with HuggingFace auth
#      * Detects architecture (LlamaForCausalLM, DeepseekForCausalLM, etc.)
#      * Sets model tokens (2048 for TinyLlama, 4096/8192 for larger models)
#      * Creates config.json and tokenizer files
#      * Sets EZLOCALAI_MODEL and DEFAULT_MODEL
#      * Fallback to proven models if requested not available
#
# üîë USER CONTROL:
#    - Temperature/Top_P: Your inference preferences (0.3 optimal for automation)
#    - Threads/GPU: Your hardware configuration  
#    - Voice/Whisper: Your audio preferences
#    - Features: IMG_ENABLED, AUTO_UPDATE, etc.
#
# üîë SECURITY:
#    - Auto-generated secure API key for JWT authentication
#    - HuggingFace token for authenticated model downloads
#
# üåê PROXY SETUP:
#    - Frontend: https://agixtui.locod-ai.com ‚Üí http://agixtinteractive:3437
#    - Backend: https://agixt.locod-ai.com ‚Üí http://agixt:7437
#    - EzLocalAI API: http://162.55.213.90:8091
#    - EzLocalAI UI: http://162.55.213.90:8502
#
# üéØ AUTOMATION OPTIMIZATION:
#    - Change MODEL_NAME to any supported model
#    - TinyLlama perfect for: N8N workflows, Camunda BPMN, deployment scripts
#    - Update PROXY URLS for your domains
#    - Modify INSTALL_BASE_PATH for your server
#    - Adjust THREADS/GPU_LAYERS for your hardware
#    - UVICORN_WORKERS=3 optimized for 16GB RAM servers
#    - Script adapts everything else automatically
#
# ü§ñ AUTOMATION MODEL ALTERNATIVES:
#    MODEL_NAME=tinyllama-1.1b-chat-v1.0     # Ultra-light (800MB) - RECOMMENDED
#    MODEL_NAME=phi-2                        # Light (1.5GB) - More capable
#    MODEL_NAME=deepseek-coder-1.3b-instruct # Code-focused (3GB+) - Heavy
#    MODEL_NAME=llama-2-7b-chat              # General (6GB+) - Very heavy
#    MODEL_NAME=mistral-7b-instruct-v0.1     # Instruction (4GB+) - Heavy
#
# üöÄ V1.7 IMPROVEMENTS:
#    - Ultra-light model for better resource efficiency
#    - Optimized for automation and workflow tasks
#    - Reduced UVICORN_WORKERS from 6 to 3
#    - Enhanced for N8N, Camunda, and deployment automation
#    - Auto-generated EZLOCALAI_API_KEY for security
#    - Conversations directory auto-created
#    - Post-installation tests integrated
# =============================================================================
